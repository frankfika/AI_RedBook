# **一、大模型是什么？**

## **1、基础定义**

**大模型可以看作「数字大脑」，就像人类通过阅读和学习获取知识，大模型通过对海量文本、代码和图像（训练数据）的学习，逐渐掌握认知、理解和推理能力**。神经网络架构模拟了人脑的神经连接，使其能够处理复杂的语言和视觉任务。这种学习方式让模型能够理解语言的细微差别、提取知识并尝试解决问题，就像人类大脑在成长过程中不断积累经验与智慧。

简单一点的比喻，他就像一个天才高中生，什么都懂一点，但是因为没有经历过系统的大学和社会教育，所以什么都不完全懂。有时候会出错，有时候会胡乱答。但是你一旦给他灌输专业领域的知识，因为他的大脑聪明，所以能够很快学会，并且应对自如。

![刚刚，中国AI大模型「监管办法」来了| 极客公园](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124198)

### **核心能力**

大模型的核心能力可以概括为三个层次：

- 语言理解（理解用户输入和问题的语义）
- 知识存储（在参数中编码大量事实性和隐式知识）
- 逻辑推理（基于已有知识进行分析、推断和解决复杂问题）

这种层次递进的能力构建，使大模型能够处理各种复杂的认知任务，从简单的文本分类到深度逻辑分析。随着模型规模的增大，这些能力会呈现出涌现特性，表现出超越单纯预训练目标的复杂认知能力。



### **参数奥秘**

参数是大模型的核心组成部分，类似于人脑中的神经连接。现代大模型的参数量已达到千亿甚至万亿级别（如GPT-4超过1.7万亿参数，DeepSeek V3拥有685亿参数）。参数数量增加使模型能够存储更多知识，理解更复杂的语义关系，处理更抽象的概念。但参数量增加也带来了计算复杂度和训练成本的指数级增长，这是大模型发展面临的重要挑战。**参数可以视为模型的"记忆容量"，越多的参数意味着模型可以记住更多的模式和知识，但同样需要更高效的架构来管理这些参数。**

### **大模型技术体系全景图**

![图片](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124092)

###  

以上是大模型技术生态的完整全景图，展示了从基础设施到应用层的各个组件及其关系。这张交互式地图可以帮助您理解大模型技术栈的各个层次及其相互依赖关系。每个层次都有其特定的组件和功能：

- **基础层 - 构建模型的地基**
	- **算力（电厂）：**大规模GPU/TPU集群，提供模型训练和推理所需的计算能力
	- **数据（原油）：**高质量、多样化的训练数据集，是模型学习的基础
	- **算法（炼油技术）：**Transformer架构及其优化变体，决定了模型的结构和学习方式
- **模型层 - 从原料到成品**
	- **预训练（造发动机）：**通过自监督学习在海量数据上训练基础模型
	- **微调（改装赛车）：**针对特定任务或领域优化模型能力
	- **部署（上路行驶）：**将训练好的模型投入实际应用环境
- **应用层 - 模型与用户的接口**
	- **提示词（方向盘）：**用户通过精心设计的指令引导模型生成所需内容
	- **RAG（GPS导航）：**将外部知识库与模型结合，提升回答准确性
	- **API（加油站）：**标准化接口，使开发者能够轻松调用模型能力



------



## **2、技术革命简史**

- ### **石器时代：规则AI（1950s-2000s）**

早期AI系统主要基于人工编写的规则和逻辑，如专家系统。这些系统类似于"机械记忆"，只能根据预设规则回答问题，缺乏自主学习能力。如同石器时代的工具，功能单一且有明确界限，无法适应复杂多变的环境。**这个阶段的AI依赖于符号主义和逻辑推理，对问题的处理方式是"如果...那么..."的规则集合，处理复杂场景时效率低下且维护成本高昂。**



### **蒸汽时代：深度学习（2006-2016）**

### **![Machine Learning 學習筆記(1) — 基本概念、Model Training 常見問題與解法| by Zola ☀️ | Medium](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175123989)**

深度学习的兴起标志着AI进入"蒸汽时代"。深度神经网络能够从数据中自动学习特征和模式，而不是依赖人工规则。这个时期的突破包括卷积神经网络（CNN）在**计算机视觉中的应用和循环神经网络（RNN）在序列数据处理中的应用，**使AI系统能够在图像识别、语音识别等任务上取得长足进步。2012年，AlexNet在ImageNet比赛中的胜利被视为深度学习革命的起点，之后神经网络的深度和复杂度不断增加，为大模型时代奠定了基础。



### **信息时代：Transformer架构（2017-2020）**

### **![自注意力和Transformer架构](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124098)**

**2017年，Google研究团队发表论文《Attention is All You Need》，提出了Transformer架构，**这是AI领域的一次范式转变。Transformer摒弃了RNN的序列处理方式，采用自注意力机制并行处理序列数据，解决了长序列依赖问题，同时大幅提高了训练效率。Transformer架构为后来的BERT、GPT等模型奠定了基础，使NLP领域取得质的飞跃。自注意力机制使模型能够同时关注输入序列中的不同部分，捕捉长距离依赖关系，这种结构上的创新为大规模参数模型的训练提供了可能。



### **智能爆炸：大模型时代（2020至今）**

### **![图片](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124108)**

从GPT-3（1750亿参数）到GPT-4（估计超过1.7万亿参数），大模型的规模和能力呈指数级增长。这一阶段的特点是模型规模不断扩大，训练数据量急剧增加，以及涌现能力（Emergent Abilities）的出现—当模型达到一定规模后，自然出现了未经专门训练的能力，如少样本学习、复杂推理和创造性思维。**大模型的出现标志着AI进入了一个新的阶段，具备了更接近通用人工智能（AGI）的特性。**随着规模的扩大，模型不仅在语言理解和生成上表现优异，还在数学推理、编程、常识理解等认知任务上展现出惊人能力，如DeepSeek R1能够生成高达32,000个tokens的输出，特别适合编写深度报告或分析大型数据集。





------





**二、关键概念拆解**

## **1、模型训练三部曲**

## ![AI大模型训练和微调的几个概念-智尊AI大模型社区](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124097)

### **炼钢（预训练）**

预训练就像冶炼钢铁的过程，是大模型训练的第一步。模型通过处理互联网上的海量文本、代码、图像等数据，学习语言的基本规律和世界知识。**这个阶段通常需要数千台GPU运行数月，消耗大量电力和计算资源，相当于炼钢需要高温熔炉和大量原料。**预训练的结果是一个具有基础认知能力但尚未专业化的"通用大脑"，就像优质钢材，有良好的基础性能，但尚未成型为特定零件。这一阶段决定了模型的基础性能上限，就像钢材的纯度和强度决定了最终产品的质量。



### **造零件（微调）**

微调相当于将通用钢材加工成特定零件的过程。在这个阶段，研究人员使用特定领域的高质量数据（如法律文本、医学文献、编程代码等）对预训练模型进行针对性训练，使其适应特定任务。微调可以大幅提升模型在特定领域的表现，例如，通过医学数据微调的模型可以回答专业医学问题，而法律微调的模型则擅长理解法律术语和案例。**微调阶段的计算资源需求远低于预训练，但需要高质量的标注数据和专业知识指导，就像精密零件加工需要专业工艺和质量控制。**DeepSeek提供了微调脚本，支持使用DeepSpeed进行训练，也支持使用4/8-bits qlora进行微调。



### **组装车（部署）**

部署是将训练好的大模型投入实际应用的过程，就像将各个零部件组装成完整汽车并上路行驶。在这个阶段，工程师需要解决模型在实际环境中的各种技术挑战，包括推理性能优化、服务扩展性、安全防护等。为了满足实时响应的需求，部署阶段常常需要进行模型量化、蒸馏或其他优化技术，减小模型体积并提高推理速度。同时，还需要构建周边系统，如用户界面、监控系统、日志分析等，确保模型稳定可靠地为用户提供服务。DeepSeek MoE 16B可部署在具有40GB内存的单个GPU上，无需量化，显示了其在部署效率上的优势。



------



## 2、**核心组件解析**

### **Transformer引擎**

Transformer是大模型的核心架构，就像汽车的发动机系统。其最关键的创新是"注意力机制"（Attention Mechanism），允许模型在处理文本时关注相关的上下文信息，而不是依序处理。这就像驾驶员能够同时关注道路、交通标志和其他车辆，而非只能看一个方向。

![深入了解Transformer：先进语言模型背后的两个强大引擎_transformer引擎-CSDN博客](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124038)

注意力机制通过计算输入序列中每个元素与其他所有元素的关联度，形成动态权重，使模型能够捕捉长距离依赖和复杂的语义关系。这种并行计算的方式大幅提高了模型处理长文本的能力和训练效率，是大模型性能提升的关键因素。Transformer架构使模型能够理解语言的上下文关系，区分同一词在不同语境中的含义，大大提升了语言理解的准确性。



### **Token密码**

Token是大模型处理文本的基本单位，类似于汽车零件中的螺丝钉或齿轮。在处理文本前，模型会将文本拆分成一系列token，每个token可能是一个单词、一个词的一部分、一个标点符号或特殊字符。**例如，"人工智能"可能被拆分为"人工"和"智能"两个token。**

![图片](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124088)

不同语言的token化效率不同，英文单词通常需要1-2个token，而中文字符通常每个字约消耗1个token（如"中国"消耗2个token）。当然不同的模型对于token的理解也不同，**所以会经常发现同一个单词有的模型消耗2个token，有的消耗10个。所以不要单纯只看token的价格，更要看模型对于你所需要的语言的理解程度。**

token的数量直接影响模型处理文本的效率和成本，也决定了模型能处理的上下文窗口大小。DeepSeek模型能够一次生成高达32,000个tokens，这使它特别适合编写深度报告或分析大型数据集，显著超过早期模型的能力范围。

###  

### **推理速度**

推理速度是衡量大模型性能的重要指标，类似于汽车的加速性能。它通常以每秒处理的token数（Token/s）来衡量，决定了模型响应用户请求的快慢。影响推理速度的因素包括模型大小、硬件配置、推理优化技术等。**基本行业的基准值，差不多在10-20 token/s的时候，才会明显看不出来卡顿。**

大模型的推理过程是逐token生成的：模型每次生成一个token，然后将这个token加入上下文，用于生成下一个token。这就像汽车在行驶过程中不断做出转向、加速等决策。推理速度的优化对提升用户体验至关重要，特别是在对话场景中。DeepSeek通过算法改进和优化节省计算能力，能够以比OpenAI的ChatGPT低得多的成本生成信息，这使其在资源受限环境中具有显著优势。





------





**三、三大核心技术——「技能培养」的进阶之路**

## **1、提示词工程（Prompting）**

提示词工程是一门通过优化输入语言来引导大模型产生高质量输出的技术，就像精心设计驾驶指令让汽车按需行驶。它包括艺术性和技术性两个层面，需要对模型行为有深入理解。高效的提示词能够帮助模型理解用户意图，在复杂任务中实现更好的表现。这项技术是人类与AI互动的桥梁，能够显著提升模型的实用性。

![一文彻底搞懂大模型- Prompt Engineering（提示工程），零基础入门到精通_大模型中prompt关系图-CSDN博客](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124131)

**提示词工程的核心技术：**

- **零样本提示（Zero-shot Prompting）：**直接让模型执行任务，无需提供示例。例如："解释量子力学的基本原理。"这种方法测试模型的基础知识和理解能力，适合简单明确的任务。
- **少样本提示（Few-shot Prompting）：**提供少量示例，引导模型学习任务模式。例如提供几个问答对后，模型能更准确地按照相同格式回答新问题。这种方法通过示例让模型快速适应特定任务的要求，在专业领域尤其有效。
- **思维链提示（Chain-of-Thought）：**引导模型逐步思考，展示推理过程。特别适合解决复杂数学问题或逻辑推理任务。这种方法模拟人类的思考过程，大幅提升模型在复杂推理任务中的准确性。
- **角色扮演提示：**让模型扮演特定角色（如"你是一位经验丰富的医生"），提升特定领域回答的专业性。通过给模型设定身份和背景，能够调整其回答的专业度、风格和视角。

![图片](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124136)

##  

------

##  

## **2、RAG技术（外接知识库）**

### **RAG就像是电脑的外接硬盘**

### **![Retrieval-Augmented Generation (RAG) | Deepgram](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124144)**

检索增强生成（Retrieval-Augmented Generation，RAG）是一种将外部知识库与大模型结合的技术，类似于给汽车安装GPS导航系统。它解决了大模型在训练后无法获取新信息和专业知识有限的问题，使模型能够访问最新、最准确的外部信息。DeepSeek利用先进的AI算法和数据处理技术快速从大型数据集中提取洞察力，使其成为强大的知识检索和分析工具。



### **RAG技术的工作原理：**

- ### **知识库构建：将企业文档、最新研究论文、产品手册等信息转化为向量形式，存储在向量数据库中。这个过程包括文本分割、特征提取和索引构建，使系统能够快速定位相关信息。**

- ### **智能检索：当用户提问时，系统首先在知识库中检索相关信息，通过语义相似度找到最相关的文档片段。DeepSeek配备了自然语言处理(NLP)技术，通过考虑意义和上下文来分析用户查询，确保相关搜索结果。**

- ### **增强生成：将检索到的信息与用户问题一起注入大模型的提示中，使模型能够基于这些外部知识生成答案。这种方式结合了知识库的准确性和大模型的理解能力，产生更高质量的回复。**

- ### **知识溯源：RAG系统能够标注信息来源，提高答案的可信度和透明度。这一特性使用户能够验证信息的准确性和权威性，特别适合对信息准确性要求高的场景。**

### ** **

### **RAG技术的优势：**

- 能够处理最新信息，避免大模型"知识截止日期"的限制，使模型能够获取2023年后的新数据
- 减少幻觉（Hallucination）现象，提高回答的准确性和可靠性，特别适合事实性强的专业领域
- 保护敏感信息，只需向量化存储，原始数据不直接暴露给模型，提高企业数据安全性
- 降低计算成本，无需频繁重训练模型以纳入新知识，实现知识的动态更新

![图片](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124167)



------



## **3、微调（Fine-Tune）**

微调是使用专业数据对预训练模型进行二次训练的过程，类似于对标准汽车进行专业改装以适应特定赛道或环境。**通过微调，可以让通用模型获得特定领域的专业能力，但这也可能导致模型在其他领域能力的减弱。**DeepSeek提供了先进的微调工具和技术，使开发者能够轻松地根据特定需求定制模型。

![图片](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124141)



**微调的主要方法：**

- **全量微调（Full Fine-tuning）：**更新模型的所有参数，效果最好但计算成本高昂，需要大量GPU资源。这种方法适合拥有充足计算资源的机构。
- **LoRA (Low-Rank Adaptation)：**只训练少量低秩适应参数，大幅降低计算和存储需求，是目前最流行的微调方法之一。DeepSeek支持LoRA技术，使模型能够在单个GPU上实现高效微调。
- **QLoRA：**在LoRA基础上进一步量化模型，降低内存需求，使普通消费级GPU也能进行微调。DeepSeek支持4/8-bits QLora进行微调，进一步提高了微调的可访问性。
- **Prompt Tuning：**只训练特定的提示向量，保持模型参数不变，计算效率最高但效果有限。适合资源严重受限的情况。
- **微调数据集构建**：高质量的微调数据集对成功至关重要，通常包含以下元素：

**{ "instruction": "分析以下患者的症状并给出可能的诊断", "input": "患者，男，45岁，出现胸闷、呼吸急促、左手臂疼痛，持续30分钟以上", "output": "患者症状高度疑似急性心肌梗塞...[详细诊断内容]" }**





------







**四、开源VS闭源——「安卓与iOS」的战争**

![图片](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124179)

**开源优势深度解析**

- **技术透明：**开源模型的算法和训练过程完全公开，用户可以理解模型运作机制，有助于建立信任和推动技术进步。

- **自由定制：**开发者可以根据特定需求修改模型架构、参数和训练方法，如DeepSeek的用户可以调整模型以优化特定领域性能。

- **成本控制：**虽然初期部署成本高，但长期使用成本可预测，不受供应商价格变动影响，适合大规模应用场景。

- **无知识产权限制：**开源许可允许商业使用和二次开发，创造更多创新可能性和商业机会。

	![Major Differences Between the Open and Close Sourced Software](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124175)

**闭源优势深度解析**

- **即用即开：**通过API访问，无需复杂部署和维护，大幅降低使用门槛，使非技术用户也能轻松应用AI技术。
- **持续更新：**厂商会定期提升模型性能和功能，用户可以自动获得最新版本，无需投入额外资源进行升级。
- **专业支持：**商业厂商提供技术支持和服务保障，问题解决更及时，适合企业级应用和关键业务场景。
- **合规保障：**厂商通常会处理版权、隐私等合规问题，减轻用户在法律和合规方面的压力。





------





**五、量化模型与蒸馏模型**

## **1、蒸馏模型**

**蒸馏模型**（Knowledge Distillation）是一种通过知识迁移技术将大型神经网络（教师模型）的预测能力压缩至小型模型（学生模型）的轻量化方法。其核心原理是通过软标签（Soft Targets）传递教师模型在隐式空间中学习到的特征分布与决策边界。典型流程包含三个步骤：教师模型在原始数据集上完成训练；学生模型通过拟合教师模型的输出分布（引入温度参数T软化概率分布）和学习真实标签的组合损失进行训练；最后通过剪枝量化进一步提升推理效率。

![漫画趣解：一口气搞懂模型蒸馏！ | 巨人肩膀](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124227)

与大模型（如百亿参数的GPT-3/4、PaLM等）相比，蒸馏模型主要区别体现在：

- **参数量级：**从千亿参数压缩至百万量级（缩减比例可达100-1000倍）
- **推理速度：**在NVIDIA V100显卡上，Bert-base蒸馏版推理延迟降低83%（6.2ms → 1.1ms）
- **部署场景：**可在移动端（如TensorFlow Lite模型）实现实时推理，内存占用减少90%
- **知识完整性：**保留>95%的原模型精度（如DistilBERT保持BERT 97%的GLUE分数）



------



## **2、量化模型**

量化模型是通过降低模型参数的数值精度来实现压缩和优化的技术，主要应用于深度学习模型的轻量化部署。其核心目标是在尽可能保持模型性能的前提下，显著减少存储需求和计算成本。

![科普：理解大语言模型中的模型量化- DigitalOcean中文网](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124187)

主要的原理包括

- 精度转换

	将高精度浮点数（如32位float32）转换为低精度表示（如8位int8），通过缩放因子（scale）和零点（zero point）建立映射关系。例如，将权重范围[-10,10]均匀划分为256个整数区间，每个步长约为0.078，实现浮点到整数的映射。

- **量化方法分类**

	- **对称量化：以0为中心对称分布，适用于数据分布均匀的场景，计算效率较高。**

	- **非对称量化：允许零点偏移，能更灵活地适应非对称数据分布，减少精度损失。**

	- 按粒度划分：包括逐层、逐通道量化等，其中逐通道量化因更精细的粒度通常精度更高

		

**主要技术手段包括：**

- **训练后量化（PTQ）**：直接对预训练模型进行校准和转换，无需重新训练。例如，统计激活值分布以确定量化参数，适合快速部署。
- **量化感知训练（QAT）：**在训练阶段模拟低精度计算，通过微调使模型适应量化误差，通常精度损失更小（通常<3%）。
- **极端量化：**如二值化（权重约束为{-1, +1}），可将模型尺寸缩减32倍，但精度损失较大，需结合特定优化策略

| 维度         | 大模型                  | 量化模型                |
| ------------ | ----------------------- | ----------------------- |
| 参数存储     | FP32（4字节/参数）      | INT8（1字节/参数）      |
| 计算效率     | 需Tensor Core支持的FP16 | 普通CPU支持的INT8指令集 |
| 能源消耗     | A100单卡功耗400W        | 移动芯片<5W             |
| 精度损失     | Baseline                | 通常<3%（LLM量化）      |
| 典型应用场景 | 云端推理服务器          | 边缘设备（如RK3588）    |

值得注意的是，**当前前沿的QLORA技术已实现将650亿参数模型量化至48GB GPU显存运行，相比原生FP16降低75%显存需求，这标志着量化技术正在突破大模型部署的硬件边界。**

**
**



------





**六、DeepSeek崛起密码——「特种兵训练法」**

##  

![Building AI Solutions with DeepSeek: A Hands-On Workshop - Association of  Data Scientists](https://mmbiz.qpic.cn/sz_mmbiz_jpg/pF9RLdqXYJXaQerVReCd5o4TWUsBhe8Jm2AbQRXZmGa2Eur9KkucPAC0p7miaxU5NawcqaLwrYph4Yt24PiawWyA/640?wx_fmt=other&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)DeepSeek是由杭州深度求索人工智能基础技术研究有限公司（由量化投资机构幻方量化创立）开发的通用人工智能（AGI）平台，自2023年7月推出以来迅速成为AI领域的标杆产品。其核心优势在于大语言模型（LLM）的研发与开源生态建设，已迭代发布多代模型：包括首个开源模型DeepSeek LLM（2024年1月）、第二代MoE架构模型DeepSeek-V2（2024年5月），以及最新开源的DeepSeek-V3（2024年12月）和性能对标OpenAI的DeepSeek-R1（2025年1月）。其中，V3模型采用创新的“辅助损失免费负载均衡策略”和节点受限路由机制，显著提升推理效率，代码能力甚至可媲美Claude 3.7 Sonnet。

![With China's DeepSeek, US tech fears red threat](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124217)

该平台支持多模态处理、联网搜索、文件解析等多样化功能，覆盖教育、医疗、办公等场景。例如教师可通过其自动生成教案、设计课堂互动，HR能实现智能简历解析与人才库管理。市场表现同样亮眼：截至2025年2月，日活用户突破3000万，累计下载量超1.1亿次，登顶全球140国应用商店榜单。



## **1、高效训练技术**

### **数据提纯**

不同于一些竞争对手的"数据饥渴"策略，DeepSeek采用高度精选的训练数据策略。就像健身教练强调"质量胜于数量"，DeepSeek专注于高价值、高质量的数据集，而非简单堆砌海量数据。这种策略有效减少了训练数据中的噪音、错误和冗余信息，提高了学习效率。

DeepSeek的数据筛选过程包括多层次过滤：首先是基于启发式规则的初筛，剔除明显低质量内容；然后是基于复杂算法的质量评分，保留高信息密度资料；最后是特定领域专家审查，确保专业领域数据的准确性。通过这种精细化数据策略，DeepSeek能够以较少的数据量达到竞争对手需要数倍数据才能达到的效果，显著降低了训练成本和时间。



### **混合专家（MoE）**

DeepSeek MoE技术采用"专家分工"策略，类似于将一个大型医院分成多个专科，每个专科医生只处理自己专长的病例，而不是让每个医生都处理所有类型的病例。这种架构允许模型将不同类型的任务分配给最擅长处理该任务的"专家子网络"，大幅提高了处理效率和精度。



具体来说，DeepSeek MoE由一个"路由器"组件和多个"专家"神经网络组成。当输入一个问题时，路由器会分析问题类型（如编程、医学、文学等），然后将问题路由到最适合的专家网络进行处理。这种分工合作的机制使得DeepSeek能够以相对较小的计算资源处理复杂多样的任务。例如，DeepSeek MoE 16B模型虽然名义上是16B参数，但实际激活的参数远少于此，却能达到与数倍大的密集模型相当的性能，体现了"用专业化替代规模化"的创新思路。



**DeepSeek MoE的核心优势：**

- **计算效率：** 在处理任何给定输入时，只激活模型中的一小部分参数，大幅降低计算资源需求
- **扩展能力：** 可以轻松添加新专家以增强特定领域能力，无需重训整个模型
- **部署友好：** 可部署在单个40GB内存的GPU上，无需复杂的分布式系统
- **性能卓越：** 在多个基准测试中表现优异，特别是在编程和推理任务上

##  

------

##  

## **2、推理优化**

### **动态计算**

DeepSeek的动态计算技术类似于汽车的智能变速系统。简单问题时，模型以"经济模式"运行，只激活必要的计算路径；复杂问题时，模型自动切换到"性能模式"，调用更多计算资源。这种灵活的资源分配机制大幅提高了推理效率，降低了平均响应时间和计算成本。

具体实现上，DeepSeek采用了自适应计算时间（ACT）和早停机制，根据问题复杂度和要求的回答精度自动调整计算深度。例如，回答"今天星期几"只需浅层处理，而回答"分析量子力学与相对论的关系"则会调用深层网络进行多轮推理。这种按需计算的方式使DeepSeek在保持高质量回答的同时，平均响应速度比竞争对手快30%以上。



### **中英双语**

DeepSeek采用真正的双语平行训练策略，而非简单的翻译叠加。这就像从小在双语环境中成长的孩子，能够自然流利地在两种语言间切换，而不是通过翻译来理解第二语言。模型在训练时就同时接触中英文资料，形成了直接的双语思维能力。

![DeepSeek中英版本实测：一个问题，两种答案– DW – 2025年2月2日](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124223)

这种原生双语能力使DeepSeek在处理中文时不会出现"翻译腔"，能够理解中文特有的表达方式、文化背景和语言习惯。**同时，模型也保持了英文处理的高水准，尤其在科技领域英文文献理解上表现突出。**此外，DeepSeek在处理中英混合输入时表现尤为出色，能够准确理解夹杂专业英文术语的中文文本，这对科研和技术领域的用户特别有价值。



------



## **3、生态策略**

### **开放底座**

DeepSeek采用"开源基础，服务商业"的策略，公开其基础模型代码和权重，鼓励开发者基于此构建创新应用。这种策略类似于汽车厂商提供基础引擎平台，让第三方开发者基于此开发各类专用车辆。

通过开放底座模型，**DeepSeek能够快速建立开发者社区，促进模型应用场景的多样化探索，同时收集大量反馈用于改进基础模型。这种开放式创新大大加速了模型能力的进化和应用场景的拓展，形成良性循环。**DeepSeek提供了详细的模型文档、示例代码和开发工具包，降低了开发者的使用门槛，促进生态系统的繁荣发展。



### **商业闭环**

在开放基础模型的同时，DeepSeek通过提供高级功能、专业服务和企业解决方案形成商业闭环。这类似于汽车厂商提供基本车型的同时，销售高级配置、售后服务和定制改装。

DeepSeek的商业模式包括多层次服务：**API访问（按量计费）、企业专属部署、行业解决方案和技术咨询服务。**其中特别重视为企业客户提供定制化微调服务，帮助企业基于DeepSeek底座模型构建符合特定行业需求的专业模型。这种模式既支持了开源社区的发展，又确保了公司的可持续经营，平衡了技术开放与商业价值之间的关系。

##  

------

##  

## **4、不同版本的DeepSeek**

DeepSeek-R1系列模型根据参数规模分为多个版本，涵盖从轻量级到超大规模的不同需求：

| 维度         | 轻量级（1.5B-8B）    | 中规模（14B-32B）        | 大规模（70B-671B）       |
| ------------ | -------------------- | ------------------------ | ------------------------ |
| **推理能力** | 简单任务快速响应     | 复杂逻辑推理、代码生成   | 接近人类专家的多步骤决策 |
| **部署成本** | 低（个人开发者适用） | 中等（企业级投入）       | 极高（仅大型机构承担）   |
| **实时性**   | 毫秒级延迟           | 受硬件限制，延迟中等     | 依赖云端并行计算优化延迟 |
| **场景适配** | 智能家居、移动端应用 | 企业自动化、专业工具开发 | 科研、超大规模数据分析   |

------

### **A、轻量级模型（1.5B、7B、8B）**

- 参数特性：

	  参数精简（1.5B-8B），资源占用低，适合本地化部署。

- **硬件需求：**

- 
- 消费级GPU（如RTX 3060/4060，显存4-12GB）。
- 内存需求：8GB（1.5B）至16GB（8B）。
- **适用场景：**

- 

- 轻量级智能问答、文本摘要、边缘设备语音交互。

- 个人开发工具（如语法检查、简单代码生成）。

- 个人开发者首选

- 局限性：

	复杂任务（如多轮对话、专业推理）易出现偏差。

### ** **

### **B、中规模模型（14B、32B）**

- 参数特性：

	参数量显著提升（14B-32B），逻辑推理与代码生成能力增强。

- **硬件需求：**

- 
- 高端GPU（如RTX 4090/A100，显存16-24GB）。
- 内存需求：32GB（14B）至64GB（32B）。
- **适用场景：**

- 
- 企业级智能客服、专业代码助手（支持上下文相关补全）。
- 中等规模数据分析、自动化测试。
- 中小企业首选
- **蒸馏版优化：**

- 14B/32B蒸馏版：资源消耗降低30%-40%，接近原版性能，但需优化硬件适配。

	

### **C、大规模模型（70B、671B）**

- **参数特性：**

- 
- 70B：接近GPT-4级别，支持多步骤推理与决策。
- 671B：超大规模云端计算，擅长海量数据挖掘与科研分析。
- **硬件需求：**

- 
- 70B：需2×A100或4×RTX 4090并行，内存128GB+。
- 671B：依赖8×A800集群，显存640GB+，成本约400万元。
- **适用场景：**

- 

- 科研数据建模（如基因序列分析）、战略决策支持。

- 金融预测、跨领域知识融合等高复杂度任务。

- 科研机构和高推理能力需求的单位首选

- 局限性

	：云端部署依赖网络带宽，运维成本极高。

- 蒸馏版突破

	：通过剪枝与量化，部分任务性能接近原版，但泛化能力下降。

##  

------

##  

## **5、关于KT版本的DeepSeek**

**KT版本指的是基于*****\*KTransformers框架\******的DeepSeek模型部署方案，该方案由清华大学团队提出，**旨在通过优化计算流程和硬件资源分配实现低成本、高性能的本地化模型部署。目前很多10万-20万的满血版本deepseek一体机，采用的就是kt的结构。

![DeepSeek revises privacy policy for Europe, ignores Korea's protections -  CHOSUNBIZ](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124230)

以下从技术原理、硬件需求、优缺点等维度展开详细介绍：

**KT版本的核心技术原理**

- **混合计算架构：KTransformers（简称KT）通过拆分模型计算流程，将MLA（多层潜在注意力）模块和KV-Cache（键值缓存）部署在GPU上运行，而其他模型参数（如前馈网络、路由层等）的计算任务分配到CPU内存中处理。这种设计减少了GPU显存占用，同时利用CPU并行计算能力提升整体吞吐量。**
- **KV-Cache优化：传统LLM推理需缓存历史Token的键值对（KV-Cache），占用大量显存。KT通过动态调度机制，仅将高频访问的KV-Cache保留在GPU显存中，低频部分迁移至CPU内存，并通过异步预加载技术减少数据交换延迟。\**
	\****
- **与Unsloth动态量化的结合：KT团队修改了框架源码（v0.2版本），使其支持Unsloth提出的 1.58-bit动态量化模型 该量化方案对模型不同层采用差异化精度压缩（关键层保留更高精度），在60GB内存+14GB显存的低配置下即可运行DeepSeek R1满血版。
	**

**KT版本的局限性**

- **并发能力受限**：因采用特殊计算结构，无法通过增加GPU数量提升并发量，单节点最高并发约40请求（对比Unsloth方案可达100并发）。
- **内存依赖性强：**需大容量内存支撑CPU计算，DDR4内存带宽可能成为性能瓶颈（DDR5带宽提升至6.4GT/s可缓解）。

目前部分厂商有能力通过KT版方案，将满血版671B的Deepseek部署成本压倒10万以内甚至更低。但是也要注意，KT版并发能力有限，需要注意场景的实际需求是否匹配。





------





**七、关于LLM和DeepSeek的一些问题**

##  

## **1、激活参数是什么？和普通的参数有什么不同？**

激活参数（Activation Parameters）是神经网络中与激活函数相关的可调整参数，用于控制激活函数的非线性特性。它们与普通参数（如权重和偏置）在功能、位置和数量上有显著区别。

------

### **激活参数 vs. 普通参数**

### **![如何通过激活和参数卸载来优化内存使用_月光AI博客](./%E4%B8%80%E3%80%81%E4%BB%8EDeepSeek%E7%9C%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BC%94%E8%BF%9B%E5%8F%B2.assets/640-20250506175124248)**

### A、**定义与作用**

- **激活参数**

	位于激活函数内部，控制其非线性行为。例如：

	- Leaky ReLU

		 中的负区间斜率（α）

	- PReLU

		（Parametric ReLU）中可学习的 α 参数

	- S型激活函数

		（如 Swish）中的可调节参数 β

	- 自适应激活函数

		（如通过学习动态调整形状的参数）。

- **普通参数**

	包括神经网络中的 **权重（Weights）** 和 **偏置（Biases）**，负责调整输入信号的线性组合（如 \( Wx + b \)），直接影响神经元之间的连接强度。

------

### **B、核心区别**

| **特性**         | **激活参数**                           | **普通参数**                       |
| ---------------- | -------------------------------------- | ---------------------------------- |
| **位置**         | 激活函数内部                           | 网络层的连接（权重）或偏移（偏置） |
| **数量**         | 通常较少（如每层一个参数）             | 庞大（与神经元连接数相关）         |
| **作用目标**     | 控制非线性特性（如梯度保留、输出范围） | 调整输入信号的线性组合             |
| **是否可学习**   | 可能是固定值或可学习（如 PReLU）       | 必须通过反向传播学习               |
| **对模型的影响** | 间接影响模型非线性表达能力             | 直接影响模型的信息传递与拟合能力   |

------

### ** **

### **C、具体示例**

- **激活参数场景**

	在 **PReLU** 中，每个神经元可能有独立的 α 参数，用于控制负区间的斜率。这些参数通过梯度下降优化，使激活函数动态适应数据特性。

- **普通参数场景**

	全连接层中，权重矩阵 \( W \) 的维度为 \( [输入维度, 输出维度] \)，偏置向量 \( b \) 的长度为输出维度，二者共同决定输入到输出的映射关系。

	

------

###  

### 4. **设计意义**

- **激活参数：**通过调整非线性特性，缓解梯度消失/爆炸（如 Leaky ReLU 的 α 避免神经元“死亡”），或增强模型对不同分布数据的适应性。 
- **普通参数：**作为模型的主要可学习部分，决定了网络对输入特征的提取能力和复杂模式的拟合能力。



激活参数是激活函数内部的“调控开关”，直接影响非线性表达能力；普通参数是网络的基础构建单元，负责信息传递与模式学习。两者协同工作，共同决定模型的性能。

------





------